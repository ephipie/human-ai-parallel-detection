{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ephipie/human-ai-parallel-detection/blob/main/LLM_Detection_03_Predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSGpAfCiHS4Z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import paired_cosine_distances\n",
        "\n",
        "import openai\n",
        "import random\n",
        "import math\n",
        "from google.colab import userdata\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXrXmIVqJoFd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-kYpkJpJTih"
      },
      "outputs": [],
      "source": [
        "working_df = pd.read_parquet(\"/content/drive/MyDrive/shared_data/llm_detection_data_embeds.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0qNjl7_Jg53"
      },
      "outputs": [],
      "source": [
        "working_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h_lIMTFJ1xI"
      },
      "outputs": [],
      "source": [
        "working_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vPmwUU5WhCv"
      },
      "source": [
        "## Embedding Similarity Based Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70Xlij7pJ35q"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "def add_similarity_preds(df):\n",
        "  df =   df.copy()\n",
        "  ORIG_CHUNK_1 = 'chunk_1'\n",
        "  ORIG_CHUNK_2 = 'chunk_2'\n",
        "  MODELS = ['gpt','llama']\n",
        "  base_emb = np.array(df[f\"{ORIG_CHUNK_1}_embeddings\"].tolist())\n",
        "  for candidate in (ORIG_CHUNK_2, *MODELS):\n",
        "    candidate_emb = np.array(df[f\"{candidate}_embeddings\"].tolist())\n",
        "    df[f\"{candidate}_sim\"] = 1 - paired_cosine_distances(base_emb, candidate_emb)\n",
        "  for model in MODELS:\n",
        "    # Is the similarity(original chunk1, original chunk2) >  similarity(original chunk1, model generated chunk2)\n",
        "    # i.e. is the style embeddings able to identify orginal chunk?\n",
        "    df[f\"{ORIG_CHUNK_2}_vs_{model}\"] = np.where(df[f\"{ORIG_CHUNK_2}_sim\"] > df[f\"{model}_sim\"], 1, 0)\n",
        "  return df\n",
        "\n",
        "enriched_df = add_similarity_preds(working_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1TQVBVRQXUJ"
      },
      "outputs": [],
      "source": [
        "enriched_df[['domain','chunk_2_vs_gpt','chunk_2_vs_llama']].groupby('domain').mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iem-R12wWZXx"
      },
      "source": [
        "## LLM as a Judge Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6hSgJIjQbbv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prompt_original_vs_llm(T1: str, A: str, B: str) -> str:\n",
        "    \"\"\"\n",
        "    Returns a complete prompt asking the model to identify which continuation\n",
        "    (A or B) is the genuine author-written follow-up to the source text T1.\n",
        "    \"\"\"\n",
        "    return f\"\"\"You are an expert at detecting the style of written text\n",
        "And identifying whether a text is written by a human or an AI language model.\n",
        "\n",
        "You are given an original text T1 and two possible continuations A and B.\n",
        "One of these is written by the original author of T1 and the other is generated\n",
        "by an LLM instructed to continue T1 in the same style.\n",
        "\n",
        "Your task is to decide which of two continuations (A or B) is the genuine\n",
        "author-written follow-up to a source text (T1).\n",
        "\n",
        "Input\n",
        "=====\n",
        "T1:\n",
        "{T1}\n",
        "\n",
        "Continuation A:\n",
        "{A}\n",
        "\n",
        "Continuation B:\n",
        "{B}\n",
        "\n",
        "\n",
        "Answer with a single letter A or B.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@retry(\n",
        "    stop=stop_after_attempt(5),\n",
        "    wait=wait_exponential(\n",
        "        multiplier=1,    # Base multiplier (delay starts at 1 second)\n",
        "        min=1,          # Minimum delay between retries\n",
        "        max=8           # Maximum delay between retries\n",
        "    ),\n",
        "    reraise=True  # Re-raise the exception if all retries fail\n",
        ")\n",
        "def _query_llm_with_logprobs(\n",
        "    prompt: str,\n",
        "    *,\n",
        "    client: openai.Client,\n",
        "    model: str,\n",
        ") -> tuple[str, float]:\n",
        "    \"\"\"\n",
        "    Returns ( model_choice_letter ,  P(A) ), where P(A) is the\n",
        "    soft-max-normalised probability that the LLM assigns to token \"A\".\n",
        "    \"\"\"\n",
        "\n",
        "    # print(prompt)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are a strict evaluator. Reply with a single letter: A or B.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=0,\n",
        "        max_tokens=1,\n",
        "        logprobs=True,\n",
        "        top_logprobs=2,           # returns top-2 logprobs (A & B)\n",
        "    )\n",
        "\n",
        "    top_lp = resp.choices[0].logprobs.content[0].top_logprobs\n",
        "    lp_dict = {tok.token.strip(): tok.logprob for tok in top_lp}\n",
        "\n",
        "    lp_A = lp_dict.get(\"A\", -math.inf)\n",
        "    lp_B = lp_dict.get(\"B\", -math.inf)\n",
        "    pA   = math.exp(lp_A) / (math.exp(lp_A) + math.exp(lp_B))\n",
        "\n",
        "    choice = resp.choices[0].message.content.strip().upper()  # \"A\" or \"B\"\n",
        "    return choice, pA\n",
        "\n",
        "\n",
        "# ──────────────────────────────────────────────────────────\n",
        "# 2.  main entry - adds judge columns to DataFrame\n",
        "# ──────────────────────────────────────────────────────────\n",
        "def add_llm_judge_predictions(\n",
        "    *,\n",
        "    df: pd.DataFrame,\n",
        "    client: openai.Client,\n",
        "    rival_models: list[str],\n",
        "    judge_model: str,\n",
        "    base_col: str = \"chunk_1\",\n",
        "    seed: int = 42,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    For every row: ask LLM which continuation (chunk_2 vs each rival\n",
        "    text column) is the better follow-up to base_col.\n",
        "\n",
        "    Make a copy of the df, enrich and return.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    # determine rivals automatically (everything except base & chunk_2)\n",
        "    rnd = random.Random(seed)\n",
        "    for rival in rival_models:\n",
        "        print(rival)\n",
        "        out_choice = f\"llm_chunk2_vs_{rival}\"\n",
        "        out_prob   = f\"{out_choice}_prob\"\n",
        "        df[out_choice] = \"\"\n",
        "        df[out_prob]   = np.nan\n",
        "\n",
        "        i = 0\n",
        "        for idx, row in df.iterrows():\n",
        "            i+=1\n",
        "            pair = [(\"chunk_2\", row[\"chunk_2\"]), (rival, row[rival])]\n",
        "            rnd.shuffle(pair)\n",
        "            labels = {\"A\": pair[0], \"B\": pair[1]}\n",
        "\n",
        "            prompt = prompt_original_vs_llm(T1 = row[base_col], A = labels['A'][1], B = labels['B'][1])\n",
        "            # if i == 1:\n",
        "            #   print(prompt)\n",
        "\n",
        "            try:\n",
        "                choice, pA = _query_llm_with_logprobs(prompt,model=judge_model ,client=client)\n",
        "            except Exception as exc:\n",
        "                print(f\"Error querying LLM: {exc}\")\n",
        "                df.at[idx, out_choice] = f\"error: {exc}\"\n",
        "                df.at[idx, out_prob]   = np.nan\n",
        "                continue\n",
        "\n",
        "            is_chunk2_A = labels[\"A\"][0] == \"chunk_2\"\n",
        "            is_chunk2_B = labels[\"B\"][0] == \"chunk_2\"\n",
        "\n",
        "            assert is_chunk2_A ^ is_chunk2_B, \"Ouput should be one of A or B\"\n",
        "\n",
        "            if is_chunk2_A:\n",
        "                p_chunk2 = pA\n",
        "                winner = \"chunk_2\" if choice == \"A\" else rival\n",
        "            else:\n",
        "                p_chunk2 = 1 - pA\n",
        "                winner = \"chunk_2\" if choice == \"B\" else rival\n",
        "            df.at[idx, out_choice] = winner\n",
        "            df.at[idx, out_prob]   = float(p_chunk2)\n",
        "            if i%10 == 0:\n",
        "              print(f\"Row number {i} of {len(df)} for {row['domain']}\")\n",
        "              print(winner,float(p_chunk2) )\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(df_batch, client, rival_models, judge_model, batch_num, output_folder):\n",
        "    \"\"\"Processes a single batch and saves the results.\"\"\"\n",
        "    batch_output_path = os.path.join(output_folder, f\"batch{batch_num}.parquet\")\n",
        "    if os.path.exists(batch_output_path):\n",
        "        print(f\"Batch {batch_num} already processed. Skipping.\")\n",
        "        return pd.read_parquet(batch_output_path)\n",
        "    else:\n",
        "        print(f\"Processing batch {batch_num}...\")\n",
        "        llm_judge_batch_df = add_llm_judge_predictions(\n",
        "            df=df_batch,\n",
        "            client=client,\n",
        "            rival_models=rival_models,\n",
        "            judge_model=judge_model\n",
        "        )\n",
        "        llm_judge_batch_df.to_parquet(batch_output_path)\n",
        "        print(f\"Batch {batch_num} saved to {batch_output_path}\")\n",
        "        return llm_judge_batch_df\n",
        "\n",
        "\n",
        "def process_batches(df, client, rival_models, judge_model, batch_size, output_folder):\n",
        "  # Create output folder if it doesn't exist\n",
        "  if not os.path.exists(output_folder):\n",
        "      os.makedirs(output_folder)\n",
        "\n",
        "\n",
        "  processed_batches = []\n",
        "  for i in range(0, len(enriched_df), batch_size):\n",
        "      batch_df = enriched_df.iloc[i:i + batch_size]\n",
        "      batch_num = i // batch_size + 1\n",
        "      processed_batch_df = process_batch(batch_df, client, rival_models, judge_model, batch_num, output_folder)\n",
        "      processed_batches.append(processed_batch_df)\n",
        "\n",
        "  return processed_batches\n",
        "\n"
      ],
      "metadata": {
        "id": "OQigLxOUNzf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main processing logic\n",
        "client = openai.OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "rival_models = ['gpt', 'llama']\n",
        "judge_model = 'gpt-4o'\n",
        "batch_size = 20  # Define your batch size\n",
        "output_folder = '/content/drive/MyDrive/shared_data/llm_detection_data_preds_batched'\n",
        "processed_batches = process_batches(enriched_df, client, rival_models, judge_model, batch_size, output_folder)"
      ],
      "metadata": {
        "id": "L_auE_fkPWem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1ry2W8iciyw"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import final\n",
        "final_llm_judge_df = pd.concat(processed_batches, ignore_index=True)\n",
        "print(\"All batches processed.\")\n",
        "final_llm_judge_df.to_parquet('/content/drive/MyDrive/shared_data/llm_detection_data_preds_final.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsTQdZ4bkdre"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAs7KVfZMeDdUGrrQTko4N",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}